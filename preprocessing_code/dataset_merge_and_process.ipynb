{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ed20ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hl51981\\.conda\\envs\\pyo_oracle\\python.exe\n",
      "2.2.3\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f6fd4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'raw_data/raw_2006_fish.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m fish_2006 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw_data/raw_2006_fish.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Ensure 'Date' is a datetime column\u001b[39;00m\n\u001b[32m      3\u001b[39m site_data_2006 = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mraw_data/2006_site_data.csv\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hl51981\\.conda\\envs\\pyo_oracle\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hl51981\\.conda\\envs\\pyo_oracle\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hl51981\\.conda\\envs\\pyo_oracle\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hl51981\\.conda\\envs\\pyo_oracle\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hl51981\\.conda\\envs\\pyo_oracle\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'raw_data/raw_2006_fish.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "fish_2006 = pd.read_csv(\"raw_data/raw_2006_fish.csv\")\n",
    "# Ensure 'Date' is a datetime column\n",
    "site_data_2006 = pd.read_csv(\"raw_data/2006_site_data.csv\", encoding=\"latin1\")\n",
    "invert_2006 = pd.read_csv(\"raw_data/raw_2006_invert.csv\")\n",
    "peneid_2006 = pd.read_csv(\"raw_data/raw_2006_peneid.csv\")\n",
    "raw_2016 = pd.read_csv(\"raw_data/raw_2016.csv\")\n",
    "raw_2022 = pd.read_csv(\"raw_data/raw_2022.csv\")\n",
    "raw_2023 = pd.read_csv(\"raw_data/raw_2023.csv\")\n",
    "#species_list = pd.read_csv(\"raw_data/species_list.csv\")\n",
    "species_codex = pd.read_csv(\"raw_data/species_codex.csv\")\n",
    "study_species = pd.read_csv(\"species_presence_2006_2016_2022.csv\")\n",
    "# Concatenate the three dataframes\n",
    "merged_df = pd.concat([fish_2006, invert_2006, peneid_2006], ignore_index=True)\n",
    "unique_taxa = merged_df['Taxon'].unique()\n",
    "unique_taxa_df = pd.DataFrame(unique_taxa, columns=['Taxon'])\n",
    "\n",
    "species_list = pd.read_csv(\"final_aphia_codex_edited.csv\")\n",
    "\n",
    "# Clean column names\n",
    "species_list.columns = species_list.columns.str.strip()\n",
    "species_codex.columns = species_codex.columns.str.strip()\n",
    "\n",
    "# Process 2022 data\n",
    "data_2022 = raw_2022.copy()\n",
    "data_2022[\"Year\"] = \"2022_2023\"\n",
    "data_2022 = data_2022.melt(id_vars=[\"site_date_key\", \"Year\"], var_name=\"species_code\", value_name=\"Count\")\n",
    "data_2022 = data_2022.rename(columns={\"site_date_key\": \"SampleID\"})\n",
    "data_2022 = data_2022.merge(species_list[[\"species_code\", \"valid_name\"]], on=\"species_code\", how=\"left\")\n",
    "data_2022 = data_2022.rename(columns={\"valid_name\": \"Taxon\"})\n",
    "data_2022 = data_2022[[\"Year\", \"SampleID\", \"Taxon\", \"Count\"]]\n",
    "\n",
    "# Process 2023 data using the same structure\n",
    "data_2023 = raw_2023.copy()\n",
    "data_2023[\"Year\"] = \"2022_2023\"\n",
    "data_2023 = data_2023.melt(id_vars=[\"site_date_key\", \"Year\"], var_name=\"species_code\", value_name=\"Count\")\n",
    "data_2023 = data_2023.rename(columns={\"site_date_key\": \"SampleID\"})\n",
    "data_2023 = data_2023.merge(species_list[[\"species_code\", \"valid_name\"]], on=\"species_code\", how=\"left\")\n",
    "data_2023 = data_2023.rename(columns={\"valid_name\": \"Taxon\"})\n",
    "data_2023 = data_2023[[\"Year\", \"SampleID\", \"Taxon\", \"Count\"]]\n",
    "\n",
    "# Combine both years\n",
    "# Get the union of all columns\n",
    "all_columns = set(data_2022.columns).union(set(data_2023.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add any missing columns to each dataframe and fill with 0\n",
    "for col in all_columns:\n",
    "\tif col not in data_2022.columns:\n",
    "\t\tdata_2022[col] = 0\n",
    "\tif col not in data_2023.columns:\n",
    "\t\tdata_2023[col] = 0\n",
    "\n",
    "\n",
    "# Reorder columns to be consistent\n",
    "data_2022 = data_2022[sorted(all_columns)]\n",
    "data_2023 = data_2023[sorted(all_columns)]\n",
    "\n",
    "# Concatenate the dataframes\n",
    "combined_202x_data = pd.concat([data_2022, data_2023], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f6e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process 2016 data and map abbreviated names to full names using species_codex\n",
    "doerr_to_fullname = species_list.set_index(\"doerr_name\")[\"valid_name\"].to_dict()\n",
    "data_2016 = raw_2016.copy()\n",
    "data_2016[\"Year\"] = \"2016\"\n",
    "data_2016 = data_2016.drop(columns=[\"date\", \"bay\"], errors=\"ignore\")\n",
    "data_2016 = data_2016.melt(id_vars=[\"site_code\", \"Year\"], var_name=\"Taxon\", value_name=\"Count\")\n",
    "data_2016 = data_2016.rename(columns={\"site_code\": \"SampleID\"})\n",
    "data_2016[\"Taxon\"] = data_2016[\"Taxon\"].replace(doerr_to_fullname)\n",
    "# Standardize Taxon names for matching\n",
    "data_2016[\"Taxon\"] = data_2016[\"Taxon\"].str.strip()\n",
    "\n",
    "# Identify all Palaemonetes entries (species or genus level)\n",
    "is_palaemonetes = data_2016[\"Taxon\"].str.startswith(\"Palaemonetes\")\n",
    "\n",
    "# Replace all matching taxa with the genus label\n",
    "data_2016.loc[is_palaemonetes, \"Taxon\"] = \"Palaemonetes\"\n",
    "\n",
    "# Group and re-sum in case multiple entries now share the same SampleID and Taxon\n",
    "data_2016 = (\n",
    "\tdata_2016.groupby([\"Year\", \"SampleID\", \"Taxon\"], as_index=False)\n",
    "\t.agg({\"Count\": \"sum\"})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b428b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure 'Date' is a datetime column\n",
    "site_data_2006[\"Date\"] = pd.to_datetime(site_data_2006[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "# Filter for GeneralHabitat == 'Marsh' and month == October\n",
    "marsh_fall_sites = site_data_2006[\n",
    "\t(site_data_2006[\"GeneralHabitat\"].str.strip().str.lower() == \"marsh\") &\n",
    "\t(site_data_2006[\"Date\"].dt.month == 10)\n",
    "]\n",
    "\n",
    "# Extract list of SampleNumbers to use for subsetting data_2006\n",
    "valid_sample_ids = marsh_fall_sites[\"SampleNumber\"].unique()\n",
    "# Standardize 2006 data\n",
    "\n",
    "fish_2006[\"Year\"] = \"2006\"\n",
    "peneid_2006[\"Year\"] = \"2006\"\n",
    "invert_2006[\"Year\"] = \"2006\"\n",
    "invert_2006[\"Count\"] = 1  # Assume presence\n",
    "\n",
    "\n",
    "minello_to_fullname = species_list.set_index(\"2005_name\")[\"valid_name\"].to_dict()\n",
    "\n",
    "data_2006 = pd.concat([\n",
    "\tfish_2006[[\"Year\", \"SampleNumber\", \"Taxon\", \"Count\"]],\n",
    "\tpeneid_2006[[\"Year\", \"SampleNumber\", \"Taxon\", \"Count\"]],\n",
    "\tinvert_2006[[\"Year\", \"SampleNumber\", \"Taxon\", \"Count\"]]\n",
    "], ignore_index=True)\n",
    "data_2006 = data_2006.rename(columns={\"SampleNumber\": \"SampleID\"})\n",
    "data_2006[\"Taxon\"] = data_2006[\"Taxon\"].replace(minello_to_fullname)\n",
    "data_2006 = data_2006[data_2006[\"SampleID\"].isin(valid_sample_ids)]\n",
    "# Combine all datasets\n",
    "all_years = pd.concat([data_2006, data_2016, combined_202x_data], ignore_index=True)\n",
    "all_years = all_years.dropna(subset=[\"Taxon\"])\n",
    "all_years = all_years[all_years[\"Count\"] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5405929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Abundance summary table ===\n",
    "abundance_summary = (\n",
    "\tall_years.groupby([\"Year\", \"Taxon\"], as_index=False)\n",
    "\t.agg(Total_Count=(\"Count\", \"sum\"))\n",
    "\t.pivot(index=\"Taxon\", columns=\"Year\", values=\"Total_Count\")\n",
    "\t.fillna(0).astype(int).reset_index()\n",
    ")\n",
    "\n",
    "# === Sampling effort by year ===\n",
    "sites_sampled_per_year = pd.DataFrame({\n",
    "\t\"Year\": [\"2006\", \"2016\", \"2022_2023\"],\n",
    "\t\"Unique_Sites_Sampled\": [\n",
    "\t\tdata_2006[\"SampleID\"].nunique(),\n",
    "\t\tdata_2016[\"SampleID\"].nunique(),\n",
    "\t\tcombined_202x_data[\"SampleID\"].nunique(),\n",
    "\t]\n",
    "})\n",
    "\n",
    "# === Presence summary: number of sites each taxon was detected in ===\n",
    "presence_summary = (\n",
    "\tall_years.groupby([\"Year\", \"Taxon\"])[\"SampleID\"]\n",
    "\t.nunique()\n",
    "\t.reset_index(name=\"Sites_Present\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Merge with total number of sites per year ===\n",
    "presence_summary = presence_summary.merge(\n",
    "\tsites_sampled_per_year, on=\"Year\", how=\"left\"\n",
    ")\n",
    "\n",
    "# === Calculate proportion of sites visited with presence ===\n",
    "presence_summary[\"Proportion_of_Sites\"] = (\n",
    "\tpresence_summary[\"Sites_Present\"] / presence_summary[\"Unique_Sites_Sampled\"]\n",
    ").round(3)\n",
    "presence_summary.to_csv(\"presence_summary.csv\")\n",
    "\n",
    "\n",
    "# === Pivot for viewing proportions by taxon and year ===\n",
    "presence_pivot = (\n",
    "\tpresence_summary.pivot(index=\"Taxon\", columns=\"Year\", values=\"Proportion_of_Sites\")\n",
    "\t.fillna(0)\n",
    "\t.reset_index()\n",
    ")\n",
    "\n",
    "presence_pivot.to_csv(\"presence_pivot.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80af22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    'Minuca': 'Minuca spp.',\n",
    "    'Minuca longisignalis': 'Minuca spp.',\n",
    "    'Minuca pugnax': 'Minuca spp.',\n",
    "    'Minuca rapax': 'Minuca spp.',\n",
    "    'Leander tenuicornis': 'Palaemon spp.',\n",
    "    'Palaemon intermedius': 'Palaemon spp.',\n",
    "    'Palaemon pugio': 'Palaemon spp.',\n",
    "    'Palaemon vulgaris': 'Palaemon spp.',\n",
    "    'Palaemonetes': 'Palaemon spp.'\n",
    "}\n",
    "\n",
    "# Apply replacement\n",
    "all_years['Taxon'] = all_years['Taxon'].replace(replace_dict)\n",
    "\n",
    "\n",
    "# === Merge with total number of sites per year ===\n",
    "presence_summary = presence_summary.merge(\n",
    "\tsites_sampled_per_year, on=\"Year\", how=\"left\"\n",
    ")\n",
    "\n",
    "# === Calculate proportion of sites visited with presence ===\n",
    "presence_summary[\"Proportion_of_Sites\"] = (\n",
    "\tpresence_summary[\"Sites_Present\"] / presence_summary[\"Unique_Sites_Sampled\"]\n",
    ").round(3)\n",
    "presence_summary.to_csv(\"presence_summary.csv\")\n",
    "\n",
    "\n",
    "# === Pivot for viewing proportions by taxon and year ===\n",
    "presence_pivot = (\n",
    "\tpresence_summary.pivot(index=\"Taxon\", columns=\"Year\", values=\"Proportion_of_Sites\")\n",
    "\t.fillna(0)\n",
    "\t.reset_index()\n",
    ")\n",
    "\n",
    "presence_pivot.to_csv(\"presence_pivot_merged_sp.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5586c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Optional: Sampling effort by season ===\n",
    "def infer_season(sample_id):\n",
    "\tif pd.isna(sample_id) or not isinstance(sample_id, str):\n",
    "\t\treturn \"Unknown\"\n",
    "\tif any(s in sample_id.lower() for s in [\"spr\", \"apr\", \"mar\"]):\n",
    "\t\treturn \"Spring\"\n",
    "\tif any(s in sample_id.lower() for s in [\"sum\", \"jun\", \"jul\", \"aug\"]):\n",
    "\t\treturn \"Summer\"\n",
    "\tif any(s in sample_id.lower() for s in [\"fall\", \"sep\", \"oct\", \"nov\"]):\n",
    "\t\treturn \"Fall\"\n",
    "\tif any(s in sample_id.lower() for s in [\"win\", \"jan\", \"feb\", \"dec\"]):\n",
    "\t\treturn \"Winter\"\n",
    "\treturn \"Unknown\"\n",
    "\n",
    "def extract_season_fallback(sample_id):\n",
    "\tif not isinstance(sample_id, str):\n",
    "\t\treturn \"Unknown\"\n",
    "\tif any(x in sample_id for x in [\"01\", \"02\", \"12\"]):\n",
    "\t\treturn \"Winter\"\n",
    "\tif any(x in sample_id for x in [\"03\", \"04\", \"05\"]):\n",
    "\t\treturn \"Spring\"\n",
    "\tif any(x in sample_id for x in [\"06\", \"07\", \"08\"]):\n",
    "\t\treturn \"Summer\"\n",
    "\tif any(x in sample_id for x in [\"09\", \"10\", \"11\"]):\n",
    "\t\treturn \"Fall\"\n",
    "\treturn \"Unknown\"\n",
    "\n",
    "season_effort = all_years.copy()\n",
    "season_effort[\"Season\"] = season_effort[\"SampleID\"].apply(infer_season)\n",
    "season_effort.loc[season_effort[\"Season\"] == \"Unknown\", \"Season\"] = (\n",
    "\tseason_effort.loc[season_effort[\"Season\"] == \"Unknown\", \"SampleID\"]\n",
    "\t.apply(extract_season_fallback)\n",
    ")\n",
    "\n",
    "sampling_by_season = (\n",
    "\tseason_effort.groupby([\"Year\", \"Season\"])[\"SampleID\"]\n",
    "\t.nunique().reset_index(name=\"Num_Samples\")\n",
    "\t.sort_values([\"Year\", \"Season\"])\n",
    ")\n",
    "\n",
    "# These final tables are:\n",
    "# - abundance_summary\n",
    "# - sites_sampled_per_year\n",
    "# - sampling_by_season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Optional: Sampling effort by season ===\n",
    "def infer_season(sample_id):\n",
    "\tif pd.isna(sample_id) or not isinstance(sample_id, str):\n",
    "\t\treturn \"Unknown\"\n",
    "\tif any(s in sample_id.lower() for s in [\"spr\", \"apr\", \"mar\"]):\n",
    "\t\treturn \"Spring\"\n",
    "\tif any(s in sample_id.lower() for s in [\"sum\", \"jun\", \"jul\", \"aug\"]):\n",
    "\t\treturn \"Summer\"\n",
    "\tif any(s in sample_id.lower() for s in [\"fall\", \"sep\", \"oct\", \"nov\"]):\n",
    "\t\treturn \"Fall\"\n",
    "\tif any(s in sample_id.lower() for s in [\"win\", \"jan\", \"feb\", \"dec\"]):\n",
    "\t\treturn \"Winter\"\n",
    "\treturn \"Unknown\"\n",
    "\n",
    "def extract_season_fallback(sample_id):\n",
    "\tif not isinstance(sample_id, str):\n",
    "\t\treturn \"Unknown\"\n",
    "\tif any(x in sample_id for x in [\"01\", \"02\", \"12\"]):\n",
    "\t\treturn \"Winter\"\n",
    "\tif any(x in sample_id for x in [\"03\", \"04\", \"05\"]):\n",
    "\t\treturn \"Spring\"\n",
    "\tif any(x in sample_id for x in [\"06\", \"07\", \"08\"]):\n",
    "\t\treturn \"Summer\"\n",
    "\tif any(x in sample_id for x in [\"09\", \"10\", \"11\"]):\n",
    "\t\treturn \"Fall\"\n",
    "\treturn \"Unknown\"\n",
    "\n",
    "season_effort = all_years.copy()\n",
    "season_effort[\"Season\"] = season_effort[\"SampleID\"].apply(infer_season)\n",
    "season_effort.loc[season_effort[\"Season\"] == \"Unknown\", \"Season\"] = (\n",
    "\tseason_effort.loc[season_effort[\"Season\"] == \"Unknown\", \"SampleID\"]\n",
    "\t.apply(extract_season_fallback)\n",
    ")\n",
    "\n",
    "sampling_by_season = (\n",
    "\tseason_effort.groupby([\"Year\", \"Season\"])[\"SampleID\"]\n",
    "\t.nunique().reset_index(name=\"Num_Samples\")\n",
    "\t.sort_values([\"Year\", \"Season\"])\n",
    ")\n",
    "\n",
    "# These final tables are:\n",
    "# - abundance_summary\n",
    "# - sites_sampled_per_year\n",
    "# - sampling_by_season\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process 2016 data and map abbreviated names to full names using species_codex\n",
    "doerr_to_fullname = species_list.set_index(\"doerr_name\")[\"valid_name\"].to_dict()\n",
    "data_2016 = raw_2016.copy()\n",
    "data_2016[\"Year\"] = 2016\n",
    "data_2016 = data_2016.drop(columns=[\"date\", \"bay\"], errors=\"ignore\")\n",
    "data_2016 = data_2016.melt(id_vars=[\"site_code\", \"Year\"], var_name=\"Taxon\", value_name=\"Count\")\n",
    "data_2016 = data_2016.rename(columns={\"site_code\": \"SampleID\"})\n",
    "data_2016[\"Taxon\"] = data_2016[\"Taxon\"].replace(doerr_to_fullname)\n",
    "# Standardize Taxon names for matching\n",
    "data_2016[\"Taxon\"] = data_2016[\"Taxon\"].str.strip()\n",
    "\n",
    "# Identify all Palaemonetes entries (species or genus level)\n",
    "is_palaemonetes = data_2016[\"Taxon\"].str.startswith(\"Palaemonetes\")\n",
    "\n",
    "# Replace all matching taxa with the genus label\n",
    "data_2016.loc[is_palaemonetes, \"Taxon\"] = \"Palaemonetes\"\n",
    "\n",
    "# Group and re-sum in case multiple entries now share the same SampleID and Taxon\n",
    "data_2016 = (\n",
    "\tdata_2016.groupby([\"Year\", \"SampleID\", \"Taxon\"], as_index=False)\n",
    "\t.agg({\"Count\": \"sum\"})\n",
    ")\n",
    "\n",
    "# Ensure 'Date' is a datetime column\n",
    "site_data_2006[\"Date\"] = pd.to_datetime(site_data_2006[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "# Filter for GeneralHabitat == 'Marsh' and month == October\n",
    "marsh_fall_sites = site_data_2006[\n",
    "\t(site_data_2006[\"GeneralHabitat\"].str.strip().str.lower() == \"marsh\") &\n",
    "\t(site_data_2006[\"Date\"].dt.month == 10)\n",
    "]\n",
    "\n",
    "# Extract list of SampleNumbers to use for subsetting data_2006\n",
    "valid_sample_ids = marsh_fall_sites[\"SampleNumber\"].unique()\n",
    "# Standardize 2006 data\n",
    "\n",
    "fish_2006[\"Year\"] = 2006\n",
    "peneid_2006[\"Year\"] = 2006\n",
    "invert_2006[\"Year\"] = 2006\n",
    "invert_2006[\"Count\"] = 1  # Assume presence\n",
    "\n",
    "\n",
    "minello_to_fullname = species_list.set_index(\"2005_name\")[\"valid_name\"].to_dict()\n",
    "\n",
    "data_2006 = pd.concat([\n",
    "\tfish_2006[[\"Year\", \"SampleNumber\", \"Taxon\", \"Count\"]],\n",
    "\tpeneid_2006[[\"Year\", \"SampleNumber\", \"Taxon\", \"Count\"]],\n",
    "\tinvert_2006[[\"Year\", \"SampleNumber\", \"Taxon\", \"Count\"]]\n",
    "], ignore_index=True)\n",
    "data_2006 = data_2006.rename(columns={\"SampleNumber\": \"SampleID\"})\n",
    "data_2006[\"Taxon\"] = data_2006[\"Taxon\"].replace(minello_to_fullname)\n",
    "data_2006 = data_2006[data_2006[\"SampleID\"].isin(valid_sample_ids)]\n",
    "# Combine all datasets\n",
    "all_years = pd.concat([data_2006, data_2016, data_2022], ignore_index=True)\n",
    "all_years = all_years.dropna(subset=[\"Taxon\"])\n",
    "all_years = all_years[all_years[\"Count\"] > 0]\n",
    "\n",
    "# === Abundance summary table ===\n",
    "abundance_summary = (\n",
    "\tall_years.groupby([\"Year\", \"Taxon\"], as_index=False)\n",
    "\t.agg(Total_Count=(\"Count\", \"sum\"))\n",
    "\t.pivot(index=\"Taxon\", columns=\"Year\", values=\"Total_Count\")\n",
    "\t.fillna(0).astype(int).reset_index()\n",
    ")\n",
    "\n",
    "# === Sampling effort by year ===\n",
    "sites_sampled_per_year = pd.DataFrame({\n",
    "\t\"Year\": [2006, 2016, 2022],\n",
    "\t\"Unique_Sites_Sampled\": [\n",
    "\t\tdata_2006[\"SampleID\"].nunique(),\n",
    "\t\tdata_2016[\"SampleID\"].nunique(),\n",
    "\t\tdata_2022[\"SampleID\"].nunique()\n",
    "\t]\n",
    "})\n",
    "\n",
    "# === Presence summary: number of sites each taxon was detected in ===\n",
    "# Mapping dictionary\n",
    "replace_dict = {\n",
    "    'Minuca': 'Minuca spp.',\n",
    "    'Minuca longisignalis': 'Minuca spp.',\n",
    "    'Minuca pugnax': 'Minuca spp.',\n",
    "    'Minuca rapax': 'Minuca spp.',\n",
    "    'Leander tenuicornis': 'Palaemon spp.',\n",
    "    'Palaemon intermedius': 'Palaemon spp.',\n",
    "    'Palaemon pugio': 'Palaemon spp.',\n",
    "    'Palaemon vulgaris': 'Palaemon spp.',\n",
    "    'Palaemonetes': 'Palaemon spp.'\n",
    "}\n",
    "\n",
    "# Apply replacement\n",
    "all_years['Taxon'] = all_years['Taxon'].replace(replace_dict)\n",
    "\n",
    "\n",
    "presence_summary = (\n",
    "\tall_years.groupby([\"Year\", \"Taxon\"])[\"SampleID\"]\n",
    "\t.nunique()\n",
    "\t.reset_index(name=\"Sites_Present\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# === Merge with total number of sites per year ===\n",
    "presence_summary = presence_summary.merge(\n",
    "\tsites_sampled_per_year, on=\"Year\", how=\"left\"\n",
    ")\n",
    "\n",
    "# === Calculate proportion of sites visited with presence ===\n",
    "presence_summary[\"Proportion_of_Sites\"] = (\n",
    "\tpresence_summary[\"Sites_Present\"] / presence_summary[\"Unique_Sites_Sampled\"]\n",
    ").round(3)\n",
    "presence_summary.to_csv(\"presence_summary.csv\")\n",
    "\n",
    "# === Pivot for viewing proportions by taxon and year ===\n",
    "presence_pivot = (\n",
    "\tpresence_summary.pivot(index=\"Taxon\", columns=\"Year\", values=\"Proportion_of_Sites\")\n",
    "\t.fillna(0)\n",
    "\t.reset_index()\n",
    ")\n",
    "\n",
    "presence_pivot.to_csv(\"presence_pivot.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
